---
title: "Exploring mobility dynamics in FVG - part 1: data preparation"
format:
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
editor: visual
---

# Context and objectives

The dataset consists of information on mobility within the Region and
includes the following information: origin and destination, number of
journeys, date, time and type of journey. Specifically, each entry in
the dataset represents a group of journeys undertaken by individuals.

The dataset also includes date and time: journeys are grouped by day and
additional columns provide information to calculate flows by blocks of
three hours. Additionally, the 'type' attribute distinguishes between
residents and travellers, and (only for residents) between inbound and
outbound journeys.

The purpose of this notebook is data preparation and exploration of the
dataset 'mobilityFVG'.

The main output is a tidy dataset in `.csv` format.

# Data preparation

The dataset is provided as multiple Excel files (.xlsx format), and
requires cleaning for analysis. Tasks include:

-   Consolidating multiple xlsx files into a single .csv file ensures
    data consistency and facilitates streamlined analysis

-   Renaming variables to English

-   Removing special characters and whitespaces

-   Derive categorical variables from the 'type' column, simplifying
    them into more efficient categories such as 'inbound' and 'outbound'

-   Set appropriate date format, and calculate weekday (1 = Monday) and
    Month (1 to 12).

-   Eliminating unnecessary columns.

The R code utilizes the \`readxl\` package to import data from Excel
files and the \`tidyverse\` package for efficient data manipulation.
Specifically, tidyverse\` an efficient syntax and verbs like \`mutate\`,
\`rename\`, and \`select\` to clean and preprocess the dataset,
enhanching readability and facilitating code debug and reuse.

```{r}
#| echo: true
#| warning: false
library(tidyverse) 
library(readxl)
library(gridExtra)
```

Data sources for this notebook

```{r}
directory_path = './data'
csv_files <- list.files(path = directory_path, pattern = "\\.csv$", full.names = TRUE , recursive = TRUE)
csv_files
```

```{r}

input_files <- c('./data/EXPORT FVG _ 6-10 MARZO 2023.xlsx',
                  './data/EXPORT FVG _ 11-15 MARZO 2023.xlsx',
                  './data/EXPORT FVG _ 1-5 GIUGNO 2023.xlsx',
                  './data/EXPORT FVG _ 6-10 GIUGNO 2023.xlsx')

```

Parameters for data preparation: columns to keep and their new names

```{r}
column_names <- c( "Giorno" ,
                   "Comune di partenza", "Area di censimento di partenza",
                   "Comune di arrivo", "Area di censimento di arrivo" ,
                   "Provenienza", "Tipologia viaggiatore", "Viaggi",
                   "Italiani", "Stranieri",
                   "Età 18-24", "Età 25-34", "Età 34-45", 
                   "Età 45-54", "Età 55-64", "Età 65+", 
                   "00-03", "03-06", "06-09", "09-12", 
                   "12-15", "15-18", "18-21", "21-24")

selected_cols <- c( "Giorno", 
                    "Comune di partenza", "Comune di arrivo", 
                    "Tipologia viaggiatore", 
                    "Viaggi")

new_column_names <- c( "day", "origin", "destination", "type", "n")
```

Define a function that reads a single file and processes data:

```{r}

read_and_clean <- function(df, 
                           selected_cols, 
                           new_column_names) {
  df <- df %>%
  # remove unnecessary columns
  select(all_of(selected_cols)) %>%
  
  # rename columns
  setNames(new_column_names) %>%
  
  #replace spaces with _ in column names
  rename_with(~ gsub(" ", "_", .), everything()) %>%
  
  #remove special characters such as ò, à ...
  mutate(across(where(is.character), ~ gsub("[^a-zA-Z0-9\\s]", " ", .))) %>%

  # encode and filter type of traveller
  mutate(res_trav = case_when(
    grepl("Residente", type) ~ "resident",
    grepl("Viaggiatore", type) ~ "traveller",
    TRUE ~ "--")) %>%

      
  # encode direction and resident_in / connected_with
  mutate(direction = case_when(
    grepl("di ritorno da altro Comune", type) ~ "back",
    grepl("diretto in altro Comune", type) ~ "go",
    TRUE ~ "--")) %>%
      
  mutate(resident_in = if_else(res_trav =="traveller", "--", 
                       if_else(direction == "go",origin, destination))) %>%
  mutate(connected_with = if_else(res_trav =="traveller", "--", 
                        if_else(direction == "go", destination, origin)))%>%

      #set appropriate format for Day and get weekday
  mutate(day = as.Date(day, format = "%d-%m-%Y")) %>%
  mutate(weekday = as.numeric(format(day, "%u"))) %>%
  mutate(month = month(day)) %>%
  select(day, weekday, month, origin, destination, resident_in, connected_with, direction, res_trav, n)
  return(df) 
}

```

apply the function to each file and save in a single `dataframe`

```{r}
data <- data.frame()
for (filename in input_files){
    print(paste("Reading and processing", filename))
    df <- read_excel(filename)
    if( all(column_names == colnames(df)) ){ 
        print("Colnames are OK, adding data.")
        data <- rbind(data, 
                      read_and_clean(df, selected_cols, new_column_names))
    } else {
        print("Column names do not match: skipping this file!")
        print(colnames(df))
    }
}
glimpse(data, width = 60)

```

# Add location info and short names

This section aims to establish short location names, and enrich the data
on the *origin* and *destination* with latitude, longitude, area, and
province according to the ISTAT data on *comuni*.

We must take into account that some locations in the dataset are
composed of two *comuni*, hence we need to summarize the information:

-   mean Latitude and, longitude

-   sum of area

-   Name and province of the first enrty in the list.

```{r}
comuni<-read_excel('./data/comuniFVG.xlsx') %>%
    arrange(location, nome)

#check duplicates ie locations
duplicate_locations <- comuni %>%
  group_by(location) %>% 
    arrange(location)%>%
  filter(n() > 1) 

print(duplicate_locations)

```

Group and summarize locations

```{r}
locs <- comuni %>% 
    group_by(location) %>%
    summarize(loc = first(nome_breve),
              LAT = round(mean(LAT),4),
              LON = round(mean(LON),4),
              km2 = round(sum(km2),4),
              prov = first(Prov),
              codISTAT = first(codISTAT),
              codCATASTALE = first(codCATASTALE))

```

```{r}
tmp <- locs %>% select(location, loc) 

data <- data %>% ungroup() %>%
    left_join(tmp, by = c("resident_in" = "location")) %>%
    left_join(tmp, by = c("connected_with" = "location")) %>%  
    select(-connected_with) %>% 
    rename(connected_with = loc.y) %>%
    select(-resident_in) %>% 
    rename(resident_in = loc.x)

#   rename(destination = loc) 
```

```{r}

length(unique(c( data$resident_in, data$connected_with )))

```

Check data: some locations are still fragmented in several "Area di
Censimento"

```{r}
check_n<-function(df, where_from, where_to, when){
  df %>%
    filter(resident_in == where_from)%>%
    filter(connected_with == where_to)%>%
    filter(day == when) %>%
        return()
}

check_n(data, "Trieste", "Muggia", as.Date("2023-03-10"))
check_n(data, "Cervignano", "Muggia", as.Date("2023-03-10"))

```

# compute resident_in and connected with

```{r}

```

# filter only residents

```{r}
data <- data %>%     
    filter(res_trav == "resident") 
glimpse(data, width = 60)
```

# aggregate by location

Some locations are divided in "Area di censimento". For the purpose of
our analysis, we aggreagate with sum(n)

```{r}
      
data<-data %>%
  group_by(day, origin, destination, resident_in, connected_with, direction, res_trav, weekday, month, n)%>%
  summarize(n = sum(n), .groups = "drop") 
glimpse(data, width = 60)

```

## calculate mean flow per day

Flows are aggregated as mean(n), grouped by edge, i.e. by resident_in
and connected_with\

```{r}

data<- data %>%
    group_by(day, weekday, month, resident_in, connected_with)%>%
    summarize(flow = mean(n, na.rm = TRUE), .groups = "drop") 
 glimpse(data, width = 60)

```

Each pair of locations is associated with 4 flows\
direction is encoded as 'resident_in - connected_with'

```{r}
samplelocs <- c("Grado" , "Trieste")
data %>% 
    select(day, resident_in, connected_with, flow)%>%
    filter(day == as.Date("2023-06-02")) %>%
    filter(resident_in %in% samplelocs & connected_with %in% samplelocs) %>%
    arrange(day, origin) %>%head(8)
 
 

```

### check time frame

```{r}
plot_ab_flow<- function(df, a,b,ymax=NA){
    title = paste("Residents in", a, "moving to and from", b)
    
    p<- df %>% 
        filter(resident_in == a & connected_with == b )%>% 
        mutate(weekend = if_else(weekday>5,"holiday", "working"))%>%
        ggplot(aes(x = day, y = flow, fill = weekend)) + 
        geom_col() +
        labs(title = title, x = "Day", y = "flow") +     
        facet_grid(~month, scales = "free_x")+
        theme_light()
    
    if(!is.na(ymax)){p <- p+  ylim(0,ymax)}
    return(p)
}
grid.arrange(
    plot_ab_flow(data, "Trieste", "Grado", ymax = 150), 
    plot_ab_flow(data, "Grado", "Trieste", ymax = 150))

```

```{r}
grid.arrange(
    plot_ab_flow(data, "Trieste", "Staranzano"), 
    plot_ab_flow(data, "Staranzano", "Trieste"))
```

## check locations

```{r}
locations <- data %>% filter(month==6)%>%
  group_by(resident_in) %>%
  summarize(y = sum(flow), .groups = "drop" ) %>%
  arrange(-y)

# Selecting top 20 locations
top_locations <- head(locations, 20)

# Creating the horizontal column plot
ggplot(top_locations, aes(x = reorder(resident_in, -y), y = y)) +
  geom_col() +
  labs(title = "Top 20 Locations by y",
       x = "Origin",
       y = "Y") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
```

```{r}
 

```

last 20

```{r}
locations %>% 
    tail(20)%>%
    ggplot(aes(x = reorder(origin, -y), y = y)) +
  geom_col() +
  labs(title = "Top 20 Locations by y",
       x = "Origin",
       y = "Y") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

# Saving data to csv

```{r}
data %>% write_csv('./data/flows.csv')
locs %>% write_csv('./data/locations.csv')
```

# Data exploration

```{r}
flows <-read_csv('./data/flows.csv')
locs <- read_csv('./data/locations.csv')


```

```{r}
unique(c(data$resident_in, data$connected_with))
 
```

```{r}
tmp <- flows %>% filter(month == 3) %>%
  group_by(connected_with) %>%
  summarize(s = sum(flow), .groups = "drop") %>%
  rename(loc = connected_with)

locs1 <- locs %>% left_join(tmp) 

plot_points <- locs1 %>%
  ggplot(aes(x = LON, y = LAT)) +
  geom_point(aes(size = s), color = "red", alpha = 0.5) +  
  theme_minimal() +
theme(legend.position = 'none')+
  theme(aspect.ratio = 1)+
  ggtitle("Locations on a map")

plot_head <- locs1 %>% 
    arrange(desc(s)) %>%
    head(10) %>%
    ggplot(aes(y = loc, x = s)) +   
    geom_col(fill = 'red', alpha= 0.5) + 
    theme_minimal() + 
    theme(aspect.ratio = 2)+
    ggtitle("top 10 locations by flow")

grid.arrange(plot_points, plot_head, ncol = 2)
```
